# Methodology
In this session, we will introduce the main algorithm — Latent Dirichlet Allocation (LDA) — applied in this project. We will discuss the core statistical foundation behind this algorithm, and we will also discuss the main principle and intuition behind this algorithm through a commonly used graph to indicate the operation of this algorithm to provide readers with a more direct understanding. 

Firstly, Latent Dirichlet Allocation, one of the most famous and commonly used topic modelling technique, is an unsupervised generative probalistic model that aims to describe or represent a collection of discrete data, such as text data, using probability distribution of latent variables socalled topics. In the context of text analysis or natural language process, LDA aims to describe or classify documents using topics. In other words, it aims to detect the latent topics within collection of topics or corpus. Moreover, Latent Dirichlet Allocation is a three-level hierarchical Bayesian model, which models or represents each document as a mixture of latent topics and each topic as a mixture of words. More specifically, it represent a document as a probability distribution of topics, and each topic is represented as a probability distribution of words. In addition, LDA use a method called bag of words, which regards each document as a vector of word frequency, and there is no sequence between words. In other words, this method only consider the occurance and frequency of each word within documents. Therefore, LDA defines the generative process of a document within a corpus into three steps. For each word within a specific document: first, randomly extract one topic from the topics distribution that represents that document; second, randomly extract one word from the word distribution that represents the topic we extracted from the topics distribution; thrid, repeat the first two steps until each word within that specific document is filled. In the next paragraph, I will provide a more formal description of the generative process of documents within corpus of the Latent Dirichlet Allocation.

As LDA is a generative probalistic model for corpus in text analysis, I will first define the basic component within a corpus. The most basic unit of a corpus is every single unique word from a vocabulary and each word has a single unique index from w={1, 2, 3...., V}. A collection or sequence of N number of words compose to a document denoted by W=(w1, w2, w3,...,wn). A collection of M number of documents compose to a corpus denoted by D={W1, W2, W3,....,Wm}. Each document within a corpus can have different number of words and different combination of words. As the second paragraph explains, a document Wm within a corpus D is represented by a probability distribution of topics, and a topics is represented by a probability distribution of words. Therefore, the generative process of each document W within a corpus D can be formally described below: 
1. For every document within a corpus, choose the number of words N within that document from poisson distribution N ~ Poisson(ξ). 
2. For every document within a corpus, choose a parameter θ that determines the probability of each topic within the multinomial probability distribution (Zn ~ Multinomial(θ)) that represents the document from a Dirichlet Distribution θ ~ Dir(α).                                                                                      
*(Noted: This means that the multinomial probability distribution of topics is not fixed for every document, rather it determined by the coefficient of dirichlet distribution α. Ιn other words, for every document, we extract a multinomial distribution of topics to represent the documents, with the probability of each topics determined by θ. On average the occurance of a specific coefficient θ for multimoial distribution is determined by the ceofficient of dirichlet distribution. The ceofficient of dirichlet distribution determines the probability distribution of each multinomial probability distribution of topics.)*
3. For each word wn within N number of words within the document:                                                                                                 
   a. Choose a topic z from the mulitinomial distribution of topics that represents the document from Zn ~ Multinomial(θ).                                   
   b. After choosing a topic z for each word, we then choose a word wn from a multinomial probability distribution of words, Wn ~ Multinomial(Φ), that represents the topic z we choose from a dirichlet distribution Φ ~ Dir(β).                                                                                                                                    
*(Noted: The multinomial probability distribution of words is both determined or conditioned by both the topic we choose and by the parameter β. More specifically, the multinomial probability distribution of words that represents a topic is not fixed, rather it determined by the coefficient of dirichlet distribution β. In other words, the dirichlet distribution with coefficient β determines the probability distribution of each multinomial distribution of words that represents a topic.)*

This whole generative process of Latent Dirichlet Allocation can be further explicitly expressed in a graph. Below shows the commonly used graphical model representation of LDA. As we previously indicate, LDA is a three-level hierarchical Bayesian model. The parameters within dirichlet distribution α and β are corpus level parameters, and these two parameters will be predetermined and only sampled once before generating a corpus. The parameters θ and Φ are document level parameters, and these two parameters will be only sampled once within each document. The variables z and w are word level variables and will be sampled every times a word is generated within a document.

![image](https://github.com/HongbinZhaoxYL/project/blob/main/截屏2022-04-24%2021.26.28.png)

Finanlly, based on the formal description of generative process of Latent Dirichlet Allocation above, we can derive a formal function to calculate the distribution of topics for a document. Given the generavtive process of Latent Dirichlet Allocation, the probability distribution with coefficient θ of given number of topics z that represents a specific document with certain number of words of index V within a corpus (or express this more formally the joint distribution of the topic distribution θ of a set of N topics z that represents a document and the word distribution Φ of a set of N words w that represents a topic) can be formally given by:   **p(θ, Φ, z, w| α, β)=Πp(θ|α)p(z|θ)p(Φ|β)p(w|Φ)** 


